providers:
  google:
    api_key_env: GEMINI_API_KEY
    models:
      gemini-2.5-pro:
        aliases: [gemini]
        max_tokens: 2000000  # Context window (2M tokens)
        max_completion_tokens: 8192
        timeout: 600
        retry_attempts: 3
        capabilities: ["text"]

  openai:
    api_key_env: OPENAI_API_KEY
    models:
      gpt-4o-mini:
        aliases: [gpt4o-mini, o4-mini, o4mini]
        max_tokens: 128000  # Context window
        max_completion_tokens: 16384  # Max output
        timeout: 300
        retry_attempts: 3
        capabilities: ["text"]
      gpt-4o:
        aliases: [gpt4o, o4]
        max_tokens: 128000  # Context window
        max_completion_tokens: 4096  # Max output
        timeout: 300
        retry_attempts: 3
        capabilities: ["text"]
      gpt-4-turbo:
        aliases: [gpt4-turbo, gpt4turbo]
        max_tokens: 128000  # Context window
        max_completion_tokens: 4096  # Max output
        timeout: 300
        retry_attempts: 3
        capabilities: ["text"]
      gpt-5:
        aliases: [gpt5]
        max_tokens: 200000  # Context window (o-series has 200K)
        max_completion_tokens: 100000  # High limit for reasoning + output
        timeout: 1500  # 25 minutes - GPT-5 is slow for large tasks
        retry_attempts: 3
        capabilities: ["text"]

  together:
    api_key_env: TOGETHER_API_KEY
    base_url: https://api.together.xyz/v1
    models:
      meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo:
        aliases: [llama, llama-3.1-70b, llama-70b]
        max_tokens: 128000  # Context window (128K tokens)
        max_completion_tokens: 4096
        timeout: 300
        retry_attempts: 3
        capabilities: ["text"]
      meta-llama/Llama-3.3-70B-Instruct-Turbo:
        aliases: [llama-3.3, llama-3.3-70b]
        max_tokens: 128000  # Context window (128K tokens)
        max_completion_tokens: 4096
        timeout: 300
        retry_attempts: 3
        capabilities: ["text"]
      deepseek-ai/DeepSeek-R1:
        aliases: [deepseek, deepseek-r1]
        max_tokens: 128000  # Context window (128K tokens)
        max_completion_tokens: 64000  # Together AI allows 64K max output
        timeout: 400
        retry_attempts: 3
        capabilities: ["text"]
      Qwen/Qwen2.5-72B-Instruct-Turbo:
        aliases: [qwen, qwen-72b, qwen2.5]
        max_tokens: 32768  # Context window (32K tokens - Turbo variant)
        max_completion_tokens: 8192  # Max output 8K
        timeout: 300
        retry_attempts: 3
        capabilities: ["text"]
      # DISABLED: mistralai/Mistral-Large-2411 - Model not available on Together AI (404 error)
      # mistralai/Mistral-Large-2411:
      #   aliases: [mistral, mistral-large]
      #   max_tokens: 8192
      #   max_completion_tokens: 4096
      #   timeout: 350
      #   retry_attempts: 3
      #   capabilities: ["text"]

      # DISABLED: nvidia/Llama-3.1-Nemotron-70B-Instruct-HF - Requires dedicated endpoint (non-serverless)
      # nvidia/Llama-3.1-Nemotron-70B-Instruct-HF:
      #   aliases: [nemotron, nemotron-70b]
      #   max_tokens: 8192
      #   max_completion_tokens: 4096
      #   timeout: 300
      #   retry_attempts: 3
      #   capabilities: ["text"]

  xai:
    api_key_env: XAI_API_KEY
    models:
      grok-4-0709:
        aliases: [grok, grok-4]
        max_tokens: 128000  # Context window (128K tokens)
        max_completion_tokens: 8192
        timeout: 500
        retry_attempts: 3
        capabilities: ["text"]

  anthropic:
    api_key_env: ANTHROPIC_API_KEY
    models:
      claude-opus-4-20250514:
        aliases: [opus, claude-opus, opus-4]
        max_tokens: 200000  # Context window (200K tokens)
        max_completion_tokens: 16384  # Max output 16K
        timeout: 600  # 10 minutes for large context
        retry_attempts: 3
        capabilities: ["text"]

defaults:
  models: [gemini-2.5-pro, gpt-4o-mini, deepseek-ai/DeepSeek-R1]
  output_dir: ./fiedler_output
